---
title: "Beneath the Surface: A Finding Nemo Character Study"
author: "Andrew Ni, Shawn Rapal"
date: "May 9, 2023"
output:
  rmdformats::readthedown:
    highlight: "kate"
---

```{r setup, include = FALSE}
# Set code chunk defaults 
# Consider adding `message = FALSE` option
knitr::opts_chunk$set(echo = FALSE) 
                      
# Set R environment options
options(knitr.kable.NA = '')

# Load packages

library(tidyverse) # data manipulation
library(tm) # text mining
library(wordcloud) # word cloud generator
library(tidytext) # text mining for word processing and sentiment analysis
library(reshape2) # reshapes a data frame
library(radarchart) # drawing the radar chart from a data frame
library(knitr) # report generation
library(wordcloud2)
library(dplyr)
library(stringr)

library(tidyr)
library(textdata)
library(igraph)
library(ggplot2)
library(ggnetwork)
library(scales)
```

# Intro

Welcome to "Beneath the Surface"! If you're a fan of the beloved Pixar classic, Finding Nemo, and are interested in analyzing the script using text analysis, you're in the right place. In this blog, we'll be taking a deep dive into the words spoken by each of the characters in the movie to uncover their personalities, motivations, and relationships with one another. Using text analysis tools, we'll be able to examine the language and patterns of speech of each character and draw insights about how they contribute to the overall narrative. Whether you're a fan of Marlin, Nemo, Crush, or any of the other lovable characters in this underwater adventure, "Beneath the Surface" is the place to be for a closer look at the story of Finding Nemo.

# Wordclouds

```{r shawn, include = FALSE}
data0 <- read_csv("data/Script_Finding_Nemo.csv")

marlin <- data0 %>%
  filter(name == "Marlin") %>% 
  mutate(line = str_replace_all(line, "[^[:alnum:][:space:]]", "")) %>%
  filter(!str_detect(line, "'"))

dory <- data0 %>%
  filter(name == "Dory") %>% 
  mutate(line = str_replace_all(line, "[^[:alnum:][:space:]]", "")) %>%
  filter(!str_detect(line, "'"))

gill <- data0 %>%
  filter(name == "Gill") %>% 
  mutate(line = str_replace_all(line, "[^[:alnum:][:space:]]", "")) %>%
  filter(!str_detect(line, "'"))

peach <- data0 %>%
  filter(name == "Nemo") %>% 
  mutate(line = str_replace_all(line, "[^[:alnum:][:space:]]", "")) %>%
  filter(!str_detect(line, "'"))

#cleanCorpus takes a corpus as an input and returns a cleaned version of it:

cleanCorpus <- function(corpus){

  corpus.tmp <- tm_map(corpus, removePunctuation) #Removes all punctuation from the corpus and saves the result to a temporary variable. 
  corpus.tmp <- tm_map(corpus.tmp, stripWhitespace) #Removes all leading and trailing whitespace from the corpus and saves the result.
  corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower)) #Converts all text in the corpus to lowercase.
  v_stopwords <- c(stopwords("english")) #Defines a vector v_stopwords that contains the default English stop words that should be removed from the corpus.
  corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords) # Removes all stop words from the corpus.
  corpus.tmp <- tm_map(corpus.tmp, removeNumbers) #Removes all numbers from the corpus.
  return(corpus.tmp) #Returns the cleaned version of the corpus as the output of the function.
}

# This function takes a text input and returns a data frame with the frequency count of the most common words in the text.
frequentTerms <- function(text){

  s.cor <- Corpus(VectorSource(text)) # Create a corpus object from the input text.
  s.cor.cl <- cleanCorpus(s.cor) # Clean the corpus by removing stop words and punctuation
  s.tdm <- TermDocumentMatrix(s.cor.cl) # Create a term-document matrix from the cleaned corpus.
  s.tdm <- removeSparseTerms(s.tdm, 0.999) # Remove sparse terms from the term-document matrix.
  m <- as.matrix(s.tdm) # Convert the term-document matrix to a regular matrix.
  word_freqs <- sort(rowSums(m), decreasing=TRUE) # Calculate the frequency count for each word in the matrix and sort them in descending order.
  dm <- data.frame(word=names(word_freqs), freq=word_freqs) # Create a data frame with the word and frequency count columns.
  return(dm)

}

top.movie.chars <- as.data.frame(sort(table(data0$name), decreasing=TRUE))[1:20,]
```

## Top Characters

```{r topchars}
ggplot(data=top.movie.chars, aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity", fill="lightblue", colour="black") +
  theme_bw() +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  labs(x="Character", y="Number of Lines")
```

## Marlin 

```{r marlin, warning=FALSE}
wordcloud2(frequentTerms(marlin$line), 
           size=1,
           color = c(rep("orange", 40), rep("white", 30), rep("black", 1)),
           backgroundColor = "lightblue",
           figPath="marlin4.pmng.png")
```

## Dory

```{r dory, warning=FALSE}
wordcloud2(frequentTerms(dory$line), 
           size=1,
           color = c(rep("darkblue", 30), rep("yellow", 30), rep("black", 1)),
           backgroundColor = "lightblue",
           figPath="dory5.png")
```


## Gill {.active}

```{r gill, warning=FALSE}
wordcloud2(frequentTerms(gill$line), 
           size=1,
           color = c(rep("black", 30), rep("white", 20), rep("yellow", 10)),
           backgroundColor = "lightblue",
           figPath="gill2.png")
```


## Peach

```{r peach, warning=FALSE}
wordcloud2(frequentTerms(peach$line), 
           size=2,
           color = c(rep("deeppink", 30), rep("pink", 20), rep("brown", 10)),
           backgroundColor = "lightblue",
           figPath="peach4.png")
```

# Sentiment Analysis

Sentiment Analysis involves analyzing the overall "feel" of a body of text. We wanted to see what the overall sentiments of the main characters were using two common sentiment databases: the AFINN Lexicon and the NRC Lexicon. While these lexicons enable powerful analysis of character's general sentiments, they are unfortunately not comprehensive and many of the words in Finding Nemo did not appear in either lexicon. 


```{r, include = FALSE}
data <- read_csv("data/Script_Finding_Nemo.csv")
afinn_lexicon <- get_sentiments("afinn")
nrc_lexicon <- get_sentiments("nrc")

character_word_freqs <- data %>% 
  #Filter out irrelevant characters like "Scene 1" or "Turtle 1"
  filter(!str_detect(name, "\\d")) %>%
  #Get individual words said by characters
  unnest_tokens(output = word, input = line) %>%
  #Remove irrelevant words
  anti_join(stop_words, by="word") %>%
  #Count the number of times each character says each word
  group_by(name) %>%
  count(word, sort = TRUE)
  
character_sentiments <- character_word_freqs %>%
  #Get the sentiments of each word
  inner_join(afinn_lexicon, by = "word") %>%
  #Get the average sentiments of the characters, and only take the top ten characters with most words spoken
  summarize(avg_sentiment = (sum(n*value) / (sum(n))), total_words = sum(n)) %>%
  #Top 10 characters by number of afinn-lexicon words said
  arrange(desc(total_words)) %>%
  head(10)





character_word_counts <- character_word_freqs %>% 
  #Only get words in the nrc lexicon
  inner_join(nrc_lexicon %>% 
               select(word) %>% 
               distinct(), by = "word") %>% 
  #get total number of words said
  group_by(name) %>% 
  summarize(total_words = sum(n)) %>%
  #Top 6 characters by words said
  arrange(desc(total_words)) %>%
  head(6)

percent_per_sentiment <- character_word_freqs %>% 
  #Get word sentiments
  inner_join(nrc_lexicon, by = "word") %>% 
  #Get total words said
  inner_join(character_word_counts, by = "name") %>%
  group_by(name, sentiment) %>% 
  #Calculate percentage of words with that sentiment, using first to remove duplicate fields due to each name-sentiment combination having its own total_words field
  summarize(sentiment_percentage = sum(n)/first(total_words), total_words = first(total_words)) %>% 
  #Get the top 5 sentiments per character
  group_by(name) %>%
  arrange(desc(sentiment_percentage)) %>%
  slice(1:5)


```

## AFINN Sentiments

The AFINN lexicon assigns each word a number from -5 to 5 according to how "negative" or "positive" a connotation it contains. We averaged these scores across all of the words said by each character, and calculated an average sentiment for each character on a scale of -5 to 5. For simplicity, we count the number of AFINN lexicon words said by each character, and only display the top 10 characters by total AFINN words said

```{r}
#Barplot the average sentiments of the characters, sorted with the most verbose character on the left
ggplot(data = character_sentiments, aes(x = reorder(name, -total_words), y = avg_sentiment)) +
  geom_bar(stat = "identity") + 
  labs(x = "Character", 
       y = "Average Sentiment",
       title = "Average Sentiments of Characters in Finding Nemo",
       subtitle = "Characters displayed in order of most to least words from left to right")
```

From these results, we can see that most characters are on average fairly positive. This makes sense given Finding Nemo's status as a PG movie intended for children. Of these characters, Peach and Crush have very positive average sentiments, which tracks with their personalities. Peach, the pink starfish, is a big-sister type character, helping Nemo when he first enters the dentist's tank. With her positive encouragement and protection, Peach's dialogue is overall very positive. On the other hand, Crush is a surfer dude type character, and his carefree lifestyle leads to largely positive dialogue. Finally, the one character with a negative overall sentiment is Bloat, the pufferfish. Bloat starts off by scaring Nemo when he first enters the dentist's office, and throughout the film is generally short-tempered and anxious, leading to an overall negative sentiment.

## NRC Sentiments

In contrast to the AFINN lexicon, the NRC lexicon categorizes words into different sentiment categories such as anticipation, fear, or joy, allowing for one word to exist in multiple categories. In this analysis, we count the number of words in each character's dialogue associated with each sentiment category, and display the top 5 sentiments in each character's dialogue. As this analysis is more complex, we only include the top 6 characters by total NRC words said.



```{r}
#Plot the top 5 sentiments for each character 
ggplot(data = percent_per_sentiment,
       #Arrange sentiments by how common they are in that character's dialogue
       aes(x = fct_reorder(sentiment, sentiment_percentage), 
           y = sentiment_percentage, fill = name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~name, ncol = 2, scales = "free") +
  labs(x = NULL,
       y = "Percentage of words with sentiment",
       title = "Top five sentiments for each character",
       subtitle = "Displays top six characters by total words said"
       ) +
  coord_flip() +
  theme(axis.text = element_text(size = 5),
        strip.text.x = element_text(size = 5))   
```

As before, we can see that Crush's dialogue is mostly positive, with over half of his words belonging to that category. Unfortunately, due to the large number of words that fit into the broad categories of "positive" and "negative," these sentiment categories figure prominently in each character's dialogue. 

# Network Centrality


```{r, include = FALSE}

#Top 10 characters by total words said
character_words <- character_word_freqs %>%
  #Get the number of words said
  group_by(name) %>%
  summarize(n=sum(n)) %>%
  #Top 10 characters
  arrange(desc(n)) %>%
  head(10)

#Get the number of interactions between each pair of characters
interactions <- data %>% 
  #Get all characters that spoke adjacent to each other in the script
  select(name) %>% mutate(name1 = lag(name)) %>% 
  #Inner join to only select the dialogues between characters in the top 10 by total words said
  inner_join(character_words, by="name") %>%
  inner_join(character_words, by=c("name1"="name")) %>%
  select(name, name1) %>%
  drop_na() %>% 
  #Get the total number of interactions between each pair of characters
  group_by(across(c(name, name1))) %>% 
  summarize(n = n()) %>% 
  #Make into an igraph
  igraph::graph_from_data_frame(directed=FALSE) 



#Calculate the degree and strength of each character in the network
nemo_stats <- tibble(name = vertex_attr(interactions, "name")
                    , degree = degree(interactions)
                    , strength = strength(interactions
                                          , weights = edge_attr(interactions, "n")))
  

  
```

## Character Interactions
```{r, warning = FALSE}
#Plot the network of interactions, with color and size of edges corresponding to the number of interactions
ggplot(data=interactions %>% ggnetwork()
       , aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(curvature = 0.3,
             angle = 30,
             aes(color = n,
                 linewidth = n)) +
  scale_color_continuous(type = "viridis"
                         , labels = label_number()) +
  geom_nodes(size=5) +
  geom_nodelabel(aes(label = name)) +
  labs(title = "Pairwise Interactions in Finding Nemo",
       color = "Number of interactions",
       size = "Number of interactions") +
  theme_blank() +
  theme(legend.position = "bottom", legend.box="vertical") + 
  guides(size = "none")
```

## Character Centrality

```{r}
#Plot the strength of each character in the network
ggplot(data = nemo_stats, aes(x = name, y = strength)) + 
  geom_bar(stat = "identity") + 
  labs(x = "Character Name", 
       y = "Strength Centrality",
       title = "Strength Centrality of Finding Nemo Characters",
       subtitle = "Displays top 10 characters by total words said")
```
  


 


